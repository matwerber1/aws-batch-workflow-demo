import * as cdk from 'aws-cdk-lib';
import { Construct } from 'constructs';
import * as ec2 from 'aws-cdk-lib/aws-ec2';
import * as ecs from 'aws-cdk-lib/aws-ecs';
import * as s3 from 'aws-cdk-lib/aws-s3';
import * as dynamodb from 'aws-cdk-lib/aws-dynamodb';
import * as ecrAsset from 'aws-cdk-lib/aws-ecr-assets';
import * as iam from 'aws-cdk-lib/aws-iam';
import * as path from 'path';
import * as batch from '@aws-cdk/aws-batch-alpha';
import * as fs from 'fs';
import { BatchWorkflowStep } from './batch-workflow-step';


interface GrantPermissionToSubmitJobForWorkflowStepProps {
    submittingPrincipal: iam.Role;
    submittedWorkflow: BatchWorkflowStep;

}


export class AwsBatchDemoStack extends cdk.Stack {
    private readonly vpc: ec2.Vpc;
    private readonly jobBucket: s3.Bucket;
    private readonly jobTable: dynamodb.Table;
    private readonly ec2LaunchTemplate: ec2.LaunchTemplate;

    constructor(scope: Construct, id: string, props?: cdk.StackProps) {
        super(scope, id, props);

        const workflowName = 'demoWorkflow';
        const ecrContainerImage = this.getEcrImageFromLocalContainerBuild('src/containers/demo/');


        this.vpc = this.createVpc();
        this.jobBucket = this.createJobBucket();
        this.jobTable = this.createJobTable();
        this.ec2LaunchTemplate = this.createEc2LaunchTemplateWithXRaySupport();
        const computeEnvironment = this.createComputeEnvironment();


        // Step 2 of 3 - Renders frames
        const renderWorkflowStep = new BatchWorkflowStep(this, 'RenderStep', {
            workflowName,
            workflowStepName: 'render',
            computeEnvironment,
            containerImage: ecrContainerImage,
            queuePriority: 1,
            jobBucket: this.jobBucket, 
            jobTable: this.jobTable, 
        });
        // Step 3 of 3 - Combines frames generated by Step 2
        const encodeWorkflowStep = new BatchWorkflowStep(this, 'EncodeStep', {
            workflowName,
            workflowStepName: 'encode',
            computeEnvironment,
            containerImage: ecrContainerImage,
            queuePriority: 1,
            jobBucket: this.jobBucket, 
            jobTable: this.jobTable, 
        });
        // Step 1 of 3 - encodes frames from step 2 into final video
        const planWorkflowStep = new BatchWorkflowStep(this, 'PlanStep', {
            workflowName,
            workflowStepName: 'plan',
            computeEnvironment,
            containerImage: ecrContainerImage,
            queuePriority: 5,
            jobBucket: this.jobBucket, 
            jobTable: this.jobTable, 
            containerDefinitionOptions: {
                environment: {
                    ENCODE_JOB_QUEUE: encodeWorkflowStep.jobQueue.jobQueueName,
                    RENDER_JOB_QUEUE: renderWorkflowStep.jobQueue.jobQueueName,
                    ENCODE_JOB_DEFINITION: encodeWorkflowStep.jobDefinition.jobDefinitionName,
                    RENDER_JOB_DEFINITION: renderWorkflowStep.jobDefinition.jobDefinitionName
                }
            }
        });

        this.grantSharedResourceAccessToWorkflowSteps();
        renderWorkflowStep.grantSubmitJob('render',planWorkflowStep.jobRole);
        encodeWorkflowStep.grantSubmitJob('encode',planWorkflowStep.jobRole);


        // Outputs visible after running AWS CDK in CloudFormation web console: 
        this.addCloudFormationOutputs({
            BatchVpc: this.vpc.vpcId,
            BatchComputeEnvironment: computeEnvironment.computeEnvironmentName,
            PlanJobDefinition: planWorkflowStep.jobDefinition.jobDefinitionName,
            PlanJobQueue: planWorkflowStep.jobQueue.jobQueueName,
            RenderJobDefinition: renderWorkflowStep.jobDefinition.jobDefinitionName,
            RenderJobQueue: renderWorkflowStep.jobQueue.jobQueueName,
            EncodeJobDefinition: encodeWorkflowStep.jobDefinition.jobDefinitionName,
            EncodeJobQueue: encodeWorkflowStep.jobQueue.jobQueueName,

        });
    }

    private grantSharedResourceAccessToWorkflowSteps() {

        this.node.children.forEach(child => {
            if (child instanceof BatchWorkflowStep) {
                // Grant job container IAM roles permission to shared S3 & DynamoDB resources: 
                this.jobBucket.grantReadWrite(child.jobRole);
                this.jobTable.grantReadWriteData(child.jobRole);
            }
        });
    }


    // Build local Dockerfile and push to ECR:
    private getEcrImageFromLocalContainerBuild(dockerfileDirectory: string) {
        return ecs.ContainerImage.fromDockerImageAsset(
            new ecrAsset.DockerImageAsset(this, 'ecrImage', {
                directory: path.join(__dirname, dockerfileDirectory),
            })
        );
    }


    private grantSubmitJobForWorkflowStep(props: GrantPermissionToSubmitJobForWorkflowStepProps) {
        const submittingRole = props.submittingPrincipal;
        const workflowStepName = removeNonAlphanumeric(props.submittedWorkflow.workflowStepName);
        submittingRole.attachInlinePolicy(
            new iam.Policy(this, `submitBatchWorkflowStep_${workflowStepName}`, {
                statements: [
                    new iam.PolicyStatement({
                        sid: `submitJobFor` + capitalize(workflowStepName) + `Step`,
                        effect: iam.Effect.ALLOW,
                        actions: [
                            'batch:SubmitJob',
                        ],
                        resources: [
                            props.submittedWorkflow.jobDefinition.jobDefinitionArn,
                            props.submittedWorkflow.jobQueue.jobQueueArn
                        ]
                    }),
                ]
            })
        );
    }


    // S3 bucket that may be used to provide inputs to or store outputs from the 
    // containers used to run steps within the Batch workflow
    private createJobBucket(): s3.Bucket {
        //TODO: consider adding bucket policy to enforce encryption at-rest with S3-SSE
        return new s3.Bucket(this, 'JobDataBucket', {
            removalPolicy: cdk.RemovalPolicy.DESTROY,
            autoDeleteObjects: true,
            versioned: false,
            enforceSSL: true,
            blockPublicAccess: s3.BlockPublicAccess.BLOCK_ALL,
            encryption: s3.BucketEncryption.S3_MANAGED,
            objectOwnership: s3.ObjectOwnership.BUCKET_OWNER_ENFORCED,
            lifecycleRules: [
                {
                    abortIncompleteMultipartUploadAfter: cdk.Duration.days(2),
                    transitions: [
                        {
                            storageClass: s3.StorageClass.INTELLIGENT_TIERING,
                            transitionAfter: cdk.Duration.days(0)
                        },
                    ]
                }
            ]
        });
    }


    // DynamoDB table that may optionally be used to facilitate passing 
    // information to, from, and between batch workflow jobs.
    private createJobTable(): dynamodb.Table {
        return new dynamodb.Table(this, 'JobTable', {
            removalPolicy: cdk.RemovalPolicy.DESTROY,
            billingMode: dynamodb.BillingMode.PAY_PER_REQUEST,
            partitionKey: {
                name: 'pk',
                type: dynamodb.AttributeType.STRING
            },
            sortKey: {
                name: 'sk',
                type: dynamodb.AttributeType.STRING
            }
        });
    }


    private createVpc(): ec2.Vpc {
        // This construct is a shorthand construct for creating a new VPC, subnets, 
        // route tables, and if you specify PUBLIC or PRIVATE_WITH_EGRESS subnets, 
        // a properly-configured Internet Gateway (IGW) and NAT Gateway (NGW), respectively: 
        const vpc = new ec2.Vpc(this, 'Vpc', {
            ipAddresses: ec2.IpAddresses.cidr('10.0.0.0/16'),
            maxAzs: 2,                         // Consider using all AZs for better chance of finding instances
            natGateways: 1,                    // Consider setting to same # as AZs for high availability
            subnetConfiguration: [
                {
                    name: 'public-subnet',
                    subnetType: ec2.SubnetType.PUBLIC
                },
                {
                    name: 'private-subnet',
                    subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS
                }
            ],
        });
        // Configure S3 and DynamoDB private VPC endpoint
        this.createVpcEndpoints(vpc);
        return vpc;
    }


    private createEc2LaunchTemplateWithXRaySupport(): ec2.LaunchTemplate {
        // https://aws.amazon.com/blogs/mt/configure-aws-x-ray-tracing-aws-batch-jobs/  
        const userdata = fs.readFileSync(
            path.join(__dirname, 'src/ec2-userdata.sh')
        );

        return new ec2.LaunchTemplate(this, 'Ec2LaunchTemplate', {
            userData: ec2.UserData.custom(userdata.toString()),
        });
    }


    private createComputeEnvironment(): batch.ManagedEc2EcsComputeEnvironment {
        const computeEnvironment = new batch.ManagedEc2EcsComputeEnvironment(this, 'Ec2BatchEnvironment', {
            enabled: true,
            replaceComputeEnvironment: true,    // Warning: do **not** change this value if also changing other values. See CDK docs. 
            terminateOnUpdate: true,
            vpc: this.vpc,
            vpcSubnets: {
                subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS
            },
            launchTemplate: this.ec2LaunchTemplate,
            spot: true,
            allocationStrategy: batch.AllocationStrategy.BEST_FIT_PROGRESSIVE,
            maxvCpus: 20,
            minvCpus: 1,
            useOptimalInstanceClasses: true,
            instanceClasses: [
                ec2.InstanceClass.M5
            ],
        });

        // Required permissions for AWS Batch: 
        // https://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html
        computeEnvironment.instanceRole?.addManagedPolicy(
            iam.ManagedPolicy.fromAwsManagedPolicyName('service-role/AmazonEC2ContainerServiceforEC2Role')
        );

        // Required for AWS X-Ray (optionally, used for distributed tracing)
        computeEnvironment.instanceRole?.addManagedPolicy(
            iam.ManagedPolicy.fromAwsManagedPolicyName('AWSXRayDaemonWriteAccess')
        );

        return computeEnvironment;
    }


    private addCloudFormationOutputs(outputs: { [key: string]: string }): void {
        Object.keys(outputs).forEach(key => {
            new cdk.CfnOutput(this, `CfnOutput_${key}`, {
                value: outputs[key],
            })
        });
    }


    // create vpc endpoint(s) for private connectivity to services like S3:
    private createVpcEndpoints(vpc: ec2.Vpc): void {
        new ec2.GatewayVpcEndpoint(this, 'S3Vpce', {
            service: ec2.GatewayVpcEndpointAwsService.S3,
            vpc,
        });
    }
}


function capitalize(inputStr:string) {
    const firstLetter = inputStr.charAt(0);
    const remainingLetters = inputStr.substring(1);
    return firstLetter.toUpperCase() + remainingLetters;
}


function removeNonAlphanumeric(str: string): string {
    return str.replace(/[^a-zA-Z0-9]/g, '');
}